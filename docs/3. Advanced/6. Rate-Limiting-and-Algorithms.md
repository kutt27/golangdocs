

{
# Rate Limiting in Go: A Comprehensive Guide

!!! abstract "Overview"
    Master rate limiting techniques in Go to control traffic flow, protect resources, and ensure fair usage. Learn to implement three fundamental rate limiting algorithms: Token Bucket, Fixed Window Counter, and Leaky Bucket. Understand their trade-offs, use cases, and how to apply them in real-world scenarios. Explore Go's native rate limiting packages and build custom implementations for fine-grained control.

!!! tip "Key Points"
    - **Rate Limiting**: Essential for protecting services from abuse, managing resources, and ensuring fair usage
    - **Token Bucket**: Allows bursts up to bucket capacity, refills at a fixed rate, ideal for bursty traffic
    - **Fixed Window Counter**: Simple implementation using time windows, but vulnerable to boundary bursts
    - **Leaky Bucket**: Smooths traffic by processing requests at a constant rate, prevents bursts entirely
    - **Go's x/time/rate**: Provides a production-ready Token Bucket implementation
    - **Distributed Rate Limiting**: Requires coordination across multiple instances using Redis or similar
    - **Trade-offs**: Each algorithm has different characteristics for burst handling, resource usage, and implementation complexity
    - **Best Practices**: Choose the right algorithm for your use case, monitor effectiveness, and adjust limits dynamically

## Understanding Rate Limiting

Rate limiting is a technique used to control the rate of incoming traffic to a service. It's crucial for preventing abuse, managing resource consumption, and ensuring fair usage among clients. Different algorithms offer different trade-offs between simplicity, burst handling, and resource efficiency.

!!! info "Rate Limiting Algorithms Comparison"
    ```mermaid
    graph TD
        A[Rate Limiting Algorithms] --> B[Token Bucket]
        A --> C[Fixed Window Counter]
        A --> D[Leaky Bucket]
        B --> E[Allows bursts]
        B --> F[Refills at fixed rate]
        C --> G[Simple implementation]
        C --> H[Vulnerable to boundary bursts]
        D --> I[Smooths traffic]
        D --> J[Constant processing rate]
        style A fill:#999,stroke:#333,stroke-width:2px,color:#000
    ```

## Token Bucket Algorithm

The Token Bucket algorithm is one of the most popular rate limiting approaches. It allows bursts of requests up to the bucket capacity while maintaining a long-term average rate.

### How Token Bucket Works

1. **Bucket**: Holds tokens up to a maximum capacity
2. **Token Generation**: Tokens are added to the bucket at a fixed rate
3. **Request Processing**: Each request consumes one token
4. **Burst Handling**: If the bucket has tokens, requests can be processed immediately
5. **Rate Limiting**: If the bucket is empty, requests must wait or be rejected

!!! example "Token Bucket Implementation"
    ```go title="token_bucket.go" linenums="1" hl_lines="8-78"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    // TokenBucket implements the token bucket algorithm
    type TokenBucket struct {
        capacity     int64       // Maximum number of tokens in the bucket
        tokens       int64       // Current number of tokens in the bucket
        refillRate   time.Duration // Time between token refills
        lastRefill   time.Time   // Time of last token refill
        mu           sync.Mutex  // Mutex for thread safety
    }

    // NewTokenBucket creates a new token bucket
    func NewTokenBucket(capacity int64, refillRate time.Duration) *TokenBucket {
        return &TokenBucket{
            capacity:   capacity,
            tokens:     capacity,
            refillRate: refillRate,
            lastRefill: time.Now(),
        }
    }

    // Allow checks if a request can be processed
    func (tb *TokenBucket) Allow() bool {
        tb.mu.Lock()
        defer tb.mu.Unlock()
        
        // Refill tokens based on elapsed time
        now := time.Now()
        elapsed := now.Sub(tb.lastRefill)
        if elapsed >= tb.refillRate {
            // Calculate how many tokens to add
            tokensToAdd := int64(elapsed / tb.refillRate)
            tb.tokens = min(tb.tokens+tokensToAdd, tb.capacity)
            tb.lastRefill = now
        }
        
        // Check if we have tokens available
        if tb.tokens > 0 {
            tb.tokens--
            return true
        }
        
        return false
    }

    // Wait waits until a token is available
    func (tb *TokenBucket) Wait() {
        for !tb.Allow() {
            time.Sleep(tb.refillRate / 10) // Sleep for a fraction of refill rate
        }
    }

    func min(a, b int64) int64 {
        if a < b {
            return a
        }
        return b
    }

    func main() {
        // Create a token bucket with capacity 5 and refill rate of 1 token per second
        bucket := NewTokenBucket(5, time.Second)
        
        // Simulate requests
        for i := 1; i <= 15; i++ {
            if bucket.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
            time.Sleep(200 * time.Millisecond)
        }
        
        fmt.Println("\nDemonstrating Wait() method:")
        // Reset bucket
        bucket = NewTokenBucket(3, 500*time.Millisecond)
        
        for i := 1; i <= 5; i++ {
            fmt.Printf("Request %d: Waiting... ", i)
            bucket.Wait()
            fmt.Println("Allowed")
        }
    }
    ```

### Using Go's x/time/rate Package

Go's standard library provides a production-ready Token Bucket implementation in the `x/time/rate` package:

!!! example "Using x/time/rate"
    ```go title="x_time_rate.go" linums="1" hl_lines="8-40"
    package main

    import (
        "fmt"
        "golang.org/x/time/rate"
        "time"
    )

    func main() {
        // Create a limiter that allows 10 requests per second with a burst of 5
        limiter := rate.NewLimiter(10, 5)
        
        // Simulate requests
        for i := 1; i <= 15; i++ {
            if limiter.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
            time.Sleep(100 * time.Millisecond)
        }
        
        fmt.Println("\nUsing Wait() method:")
        // Reset limiter
        limiter = rate.NewLimiter(2, 3)
        
        for i := 1; i <= 5; i++ {
            start := time.Now()
            err := limiter.Wait(context.Background())
            if err != nil {
                fmt.Printf("Request %d: Error: %v\n", i, err)
            } else {
                fmt.Printf("Request %d: Allowed (waited %v)\n", i, time.Since(start))
            }
        }
    }
    ```

## Fixed Window Counter Algorithm

The Fixed Window Counter algorithm divides time into fixed windows (e.g., 1 second) and counts requests within each window. It's simple to implement but can allow bursts at window boundaries.

### How Fixed Window Counter Works

1. **Time Windows**: Time is divided into fixed-size windows (e.g., 1 second)
2. **Counter**: Each window has a counter for the number of requests
3. **Request Processing**: When a request arrives, check the current window's counter
4. **Rate Limiting**: If the counter is below the limit, increment it and allow the request
5. **Window Reset**: When the window ends, the counter resets to zero

!!! example "Fixed Window Counter Implementation"
    ```go title="fixed_window.go" linums="1" hl_lines="8-80"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    // FixedWindowCounter implements the fixed window counter algorithm
    type FixedWindowCounter struct {
        limit       int           // Maximum requests per window
        windowSize  time.Duration // Size of each time window
        counters    map[int64]int // Map of window start time to request count
        mu          sync.Mutex    // Mutex for thread safety
    }

    // NewFixedWindowCounter creates a new fixed window counter
    func NewFixedWindowCounter(limit int, windowSize time.Duration) *FixedWindowCounter {
        return &FixedWindowCounter{
            limit:      limit,
            windowSize: windowSize,
            counters:   make(map[int64]int),
        }
    }

    // Allow checks if a request can be processed
    func (fwc *FixedWindowCounter) Allow() bool {
        fwc.mu.Lock()
        defer fwc.mu.Unlock()
        
        // Get current window start time
        now := time.Now()
        windowStart := now.Truncate(fwc.windowSize).UnixNano()
        
        // Clean up old windows (optional, for memory efficiency)
        for ws := range fwc.counters {
            if ws < windowStart-fwc.windowSize.Nanoseconds() {
                delete(fwc.counters, ws)
            }
        }
        
        // Get or create counter for current window
        count := fwc.counters[windowStart]
        
        // Check if we're within the limit
        if count < fwc.limit {
            fwc.counters[windowStart] = count + 1
            return true
        }
        
        return false
    }

    func main() {
        // Create a fixed window counter with limit 5 per second
        limiter := NewFixedWindowCounter(5, time.Second)
        
        // Simulate requests
        for i := 1; i <= 15; i++ {
            if limiter.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
            time.Sleep(200 * time.Millisecond)
        }
        
        fmt.Println("\nDemonstrating window boundary issue:")
        // Create a new limiter to demonstrate boundary issue
        limiter = NewFixedWindowCounter(3, 2*time.Second)
        
        // Time just before window boundary
        time.Sleep(time.Until(time.Now().Truncate(2*time.Second).Add(2*time.Second)) - 10*time.Millisecond)
        
        // Send requests around window boundary
        for i := 1; i <= 6; i++ {
            if limiter.Allow() {
                fmt.Printf("Request %d: Allowed at %v\n", i, time.Now().Format("15:04:05.000"))
            } else {
                fmt.Printf("Request %d: Denied at %v\n", i, time.Now().Format("15:04:05.000"))
            }
            time.Sleep(100 * time.Millisecond)
        }
    }
    ```

### Sliding Window Log (Improved Fixed Window)

To address the boundary issue of Fixed Window Counter, we can implement a Sliding Window Log that tracks request timestamps:

!!! example "Sliding Window Log"
    ```go title="sliding_window.go" linums="1" hl_lines="8-70"
    package main

    import (
        "container/list"
        "fmt"
        "sync"
        "time"
    )

    // SlidingWindowLog implements a sliding window log algorithm
    type SlidingWindowLog struct {
        limit      int           // Maximum requests in the window
        windowSize time.Duration // Size of the sliding window
        requests   *list.List    // List of request timestamps
        mu         sync.Mutex    // Mutex for thread safety
    }

    // NewSlidingWindowLog creates a new sliding window log
    func NewSlidingWindowLog(limit int, windowSize time.Duration) *SlidingWindowLog {
        return &SlidingWindowLog{
            limit:      limit,
            windowSize: windowSize,
            requests:   list.New(),
        }
    }

    // Allow checks if a request can be processed
    func (swl *SlidingWindowLog) Allow() bool {
        swl.mu.Lock()
        defer swl.mu.Unlock()
        
        now := time.Now()
        cutoff := now.Add(-swl.windowSize)
        
        // Remove old requests outside the window
        for e := swl.requests.Front(); e != nil; {
            next := e.Next()
            if e.Value.(time.Time).Before(cutoff) {
                swl.requests.Remove(e)
            } else {
                break
            }
            e = next
        }
        
        // Check if we're within the limit
        if swl.requests.Len() < swl.limit {
            swl.requests.PushBack(now)
            return true
        }
        
        return false
    }

    func main() {
        // Create a sliding window log with limit 5 per second
        limiter := NewSlidingWindowLog(5, time.Second)
        
        // Simulate requests
        for i := 1; i <= 15; i++ {
            if limiter.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
            time.Sleep(200 * time.Millisecond)
        }
    }
    ```

## Leaky Bucket Algorithm

The Leaky Bucket algorithm smooths out traffic by processing requests at a constant rate. It's particularly useful when you want to prevent bursts entirely and maintain a steady flow of requests.

### How Leaky Bucket Works

1. **Bucket**: Holds requests up to a maximum capacity
2. **Leak Rate**: Requests are processed (leaked) at a constant rate
3. **Request Processing**: New requests are added to the bucket if there's space
4. **Overflow**: If the bucket is full, new requests are rejected
5. **Constant Output**: The leak rate ensures a steady flow of processed requests

!!! example "Leaky Bucket Implementation"
    ```go title="leaky_bucket.go" linums="1" hl_lines="8-90"
    package main

    import (
        "container/list"
        "fmt"
        "sync"
        "time"
    )

    // LeakyBucket implements the leaky bucket algorithm
    type LeakyBucket struct {
        capacity   int           // Maximum number of requests in the bucket
        leakRate   time.Duration // Time between request processing (leaks)
        bucket     *list.List    // Queue of request timestamps
        lastLeak   time.Time     // Time of last leak
        mu         sync.Mutex    // Mutex for thread safety
    }

    // NewLeakyBucket creates a new leaky bucket
    func NewLeakyBucket(capacity int, leakRate time.Duration) *LeakyBucket {
        return &LeakyBucket{
            capacity: capacity,
            leakRate: leakRate,
            bucket:   list.New(),
            lastLeak: time.Now(),
        }
    }

    // Allow checks if a request can be added to the bucket
    func (lb *LeakyBucket) Allow() bool {
        lb.mu.Lock()
        defer lb.mu.Unlock()
        
        now := time.Now()
        
        // Process (leak) requests based on elapsed time
        elapsed := now.Sub(lb.lastLeak)
        if elapsed >= lb.leakRate {
            // Calculate how many requests to process
            leaks := int(elapsed / lb.leakRate)
            for i := 0; i < leaks && lb.bucket.Len() > 0; i++ {
                lb.bucket.Remove(lb.bucket.Front())
            }
            lb.lastLeak = now
        }
        
        // Check if we have space in the bucket
        if lb.bucket.Len() < lb.capacity {
            lb.bucket.PushBack(now)
            return true
        }
        
        return false
    }

    // Wait waits until the request can be added to the bucket
    func (lb *LeakyBucket) Wait() {
        for !lb.Allow() {
            time.Sleep(lb.leakRate / 10) // Sleep for a fraction of leak rate
        }
    }

    func main() {
        // Create a leaky bucket with capacity 5 and leak rate of 1 request per second
        bucket := NewLeakyBucket(5, time.Second)
        
        // Simulate burst of requests
        fmt.Println("Simulating burst of requests:")
        for i := 1; i <= 10; i++ {
            if bucket.Allow() {
                fmt.Printf("Request %d: Added to bucket\n", i)
            } else {
                fmt.Printf("Request %d: Bucket full, rejected\n", i)
            }
        }
        
        fmt.Println("\nWaiting for bucket to empty...")
        time.Sleep(6 * time.Second)
        
        fmt.Println("\nDemonstrating steady flow:")
        for i := 1; i <= 5; i++ {
            fmt.Printf("Request %d: Waiting... ", i+10)
            bucket.Wait()
            fmt.Println("Added to bucket")
        }
    }
    ```

## Comparing Rate Limiting Algorithms

Let's compare the three algorithms with a comprehensive example:

!!! example "Algorithm Comparison"
    ```go title="comparison.go" linums="1" hl_lines="8-120"
    package main

    import (
        "fmt"
        "time"
    )

    // We'll use the implementations from previous examples
    // TokenBucket, FixedWindowCounter, and LeakyBucket

    func simulateAlgorithm(name string, limiter interface{}, requests int, interval time.Duration) {
        fmt.Printf("\n=== %s ===\n", name)
        
        allowed := 0
        denied := 0
        
        for i := 1; i <= requests; i++ {
            var ok bool
            
            switch l := limiter.(type) {
            case *TokenBucket:
                ok = l.Allow()
            case *FixedWindowCounter:
                ok = l.Allow()
            case *LeakyBucket:
                ok = l.Allow()
            }
            
            if ok {
                allowed++
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                denied++
                fmt.Printf("Request %d: Denied\n", i)
            }
            
            time.Sleep(interval)
        }
        
        fmt.Printf("Summary: %d allowed, %d denied\n", allowed, denied)
    }

    func main() {
        // Create limiters with similar settings
        tokenBucket := NewTokenBucket(5, time.Second)       // 5 tokens, refills 1 per second
        fixedWindow := NewFixedWindowCounter(5, time.Second) // 5 requests per second
        leakyBucket := NewLeakyBucket(5, time.Second)      // Capacity 5, leaks 1 per second
        
        // Simulate 15 requests with 200ms interval
        requests := 15
        interval := 200 * time.Millisecond
        
        simulateAlgorithm("Token Bucket", tokenBucket, requests, interval)
        simulateAlgorithm("Fixed Window Counter", fixedWindow, requests, interval)
        simulateAlgorithm("Leaky Bucket", leakyBucket, requests, interval)
        
        // Test burst handling
        fmt.Println("\n=== Burst Handling Test ===")
        
        // Reset limiters
        tokenBucket = NewTokenBucket(5, time.Second)
        fixedWindow = NewFixedWindowCounter(5, time.Second)
        leakyBucket = NewLeakyBucket(5, time.Second)
        
        // Send 10 requests as quickly as possible
        burstRequests := 10
        
        fmt.Println("\nToken Bucket - Burst Test:")
        for i := 1; i <= burstRequests; i++ {
            if tokenBucket.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
        }
        
        fmt.Println("\nFixed Window Counter - Burst Test:")
        for i := 1; i <= burstRequests; i++ {
            if fixedWindow.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
        }
        
        fmt.Println("\nLeaky Bucket - Burst Test:")
        for i := 1; i <= burstRequests; i++ {
            if leakyBucket.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
        }
    }
    ```

## Distributed Rate Limiting

For distributed systems, rate limiting needs to be coordinated across multiple instances. Redis is commonly used for this purpose:

!!! example "Distributed Rate Limiting with Redis"
    ```go title="distributed_rate_limiting.go" linums="1" hl_lines="8-80"
    package main

    import (
        "context"
        "fmt"
        "github.com/go-redis/redis/v8"
        "time"
    )

    // RedisRateLimiter implements a distributed rate limiter using Redis
    type RedisRateLimiter struct {
        client     *redis.Client
        key        string
        limit      int64
        window     time.Duration
    }

    // NewRedisRateLimiter creates a new Redis-based rate limiter
    func NewRedisRateLimiter(client *redis.Client, key string, limit int64, window time.Duration) *RedisRateLimiter {
        return &RedisRateLimiter{
            client: client,
            key:    key,
            limit:  limit,
            window: window,
        }
    }

    // Allow checks if a request can be processed using Redis
    func (rl *RedisRateLimiter) Allow(ctx context.Context) (bool, error) {
        // Use Redis INCR and EXPIRE commands for fixed window counter
        pipe := rl.client.Pipeline()
        incr := pipe.Incr(ctx, rl.key)
        pipe.Expire(ctx, rl.key, rl.window)
        
        _, err := pipe.Exec(ctx)
        if err != nil {
            return false, err
        }
        
        count := incr.Val()
        return count <= rl.limit, nil
    }

    func main() {
        // Create Redis client
        rdb := redis.NewClient(&redis.Options{
            Addr:     "localhost:6379",
            Password: "", // no password set
            DB:       0,  // use default DB
        })
        
        // Test connection
        ctx := context.Background()
        _, err := rdb.Ping(ctx).Result()
        if err != nil {
            fmt.Printf("Failed to connect to Redis: %v\n", err)
            fmt.Println("This example requires a running Redis server")
            return
        }
        
        // Create rate limiter: 5 requests per 10 seconds
        limiter := NewRedisRateLimiter(rdb, "rate_limit:test", 5, 10*time.Second)
        
        // Simulate requests
        for i := 1; i <= 10; i++ {
            allowed, err := limiter.Allow(ctx)
            if err != nil {
                fmt.Printf("Request %d: Error: %v\n", i, err)
                continue
            }
            
            if allowed {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
            
            time.Sleep(1 * time.Second)
        }
    }
    ```

## Best Practices

!!! tip "Choose the Right Algorithm"
    Select the rate limiting algorithm based on your specific requirements:
    - **Token Bucket**: Best for allowing controlled bursts (e.g., API endpoints)
    - **Fixed Window**: Simple implementation for basic rate limiting (e.g., login attempts)
    - **Leaky Bucket**: Best for smoothing traffic and preventing bursts (e.g., message queues)

!!! example "Algorithm Selection"
    ```go title="algorithm_selection.go" linums="1" hl_lines="8-40"
    package main

    import (
        "fmt"
        "time"
    )

    func main() {
        // Example 1: API endpoint with burst allowance
    fmt.Println("API Endpoint - Token Bucket:")
    apiLimiter := NewTokenBucket(100, time.Second) // 100 requests per second, burst up to 100
    // Use for general API traffic
    
    // Example 2: Login attempts - strict limiting
    fmt.Println("\nLogin Attempts - Fixed Window:")
    loginLimiter := NewFixedWindowCounter(5, time.Minute) // 5 attempts per minute
    // Use for sensitive operations
    
    // Example 3: Email sending - smooth traffic
    fmt.Println("\nEmail Sending - Leaky Bucket:")
    emailLimiter := NewLeakyBucket(50, time.Second) // Process 1 email per second, queue up to 50
    // Use for outbound communication
    }
    ```

!!! tip "Implement Multiple Tiers"
    Use different rate limits for different user tiers or endpoints:

!!! example "Multi-tier Rate Limiting"
    ```go title="multi_tier.go" linums="1" hl_lines="8-50"
    package main

    import (
        "fmt"
        "time"
    )

    type UserTier string

    const (
        TierFree    UserTier = "free"
        TierPremium UserTier = "premium"
        TierEnterprise UserTier = "enterprise"
    )

    type RateLimiterFactory struct{}

    func (f *RateLimiterFactory) CreateLimiter(tier UserTier) interface{} {
        switch tier {
        case TierFree:
            return NewTokenBucket(10, time.Minute) // 10 requests per minute
        case TierPremium:
            return NewTokenBucket(100, time.Minute) // 100 requests per minute
        case TierEnterprise:
            return NewTokenBucket(1000, time.Minute) // 1000 requests per minute
        default:
            return NewTokenBucket(5, time.Minute) // Default limit
        }
    }

    func main() {
        factory := RateLimiterFactory{}
        
        // Create limiters for different tiers
        freeLimiter := factory.CreateLimiter(TierFree).(*TokenBucket)
        premiumLimiter := factory.CreateLimiter(TierPremium).(*TokenBucket)
        enterpriseLimiter := factory.CreateLimiter(TierEnterprise).(*TokenBucket)
        
        fmt.Println("Free tier limit test:")
        for i := 1; i <= 15; i++ {
            if freeLimiter.Allow() {
                fmt.Printf("Request %d: Allowed\n", i)
            } else {
                fmt.Printf("Request %d: Denied\n", i)
            }
        }
    }
    ```

!!! tip "Monitor and Adjust Limits"
    Implement monitoring to track rate limiting effectiveness and adjust limits dynamically:

!!! example "Dynamic Rate Limiting"
    ```go title="dynamic_limits.go" linums="1" hl_lines="8-60"
    package main

    import (
        "fmt"
        "sync/atomic"
        "time"
    )

    // DynamicRateLimiter adjusts limits based on system load
    type DynamicRateLimiter struct {
        baseLimit  int64
        currentLimit int64
        loadFactor float64
        lastAdjust time.Time
    }

    func NewDynamicRateLimiter(baseLimit int64) *DynamicRateLimiter {
        return &DynamicRateLimiter{
            baseLimit:     baseLimit,
            currentLimit:  baseLimit,
            loadFactor:    1.0,
            lastAdjust:    time.Now(),
        }
    }

    func (d *DynamicRateLimiter) AdjustLimit(load float64) {
        // Adjust limit based on load (0.0 = no load, 1.0 = max load)
        if load < 0.5 {
            // Low load, increase limit
            d.loadFactor = 1.5
        } else if load > 0.8 {
            // High load, decrease limit
            d.loadFactor = 0.5
        } else {
            // Normal load
            d.loadFactor = 1.0
        }
        
        newLimit := int64(float64(d.baseLimit) * d.loadFactor)
        atomic.StoreInt64(&d.currentLimit, newLimit)
        d.lastAdjust = time.Now()
    }

    func (d *DynamicRateLimiter) GetLimit() int64 {
        return atomic.LoadInt64(&d.currentLimit)
    }

    func main() {
        limiter := NewDynamicRateLimiter(100) // Base limit of 100
        
        fmt.Printf("Initial limit: %d\n", limiter.GetLimit())
        
        // Simulate high load
        limiter.AdjustLimit(0.9)
        fmt.Printf("High load limit: %d\n", limiter.GetLimit())
        
        // Simulate low load
        limiter.AdjustLimit(0.3)
        fmt.Printf("Low load limit: %d\n", limiter.GetLimit())
    }
    ```

## Real-World Example: API Rate Limiting Middleware

Let's create a comprehensive example that demonstrates rate limiting in a web API context:

!!! example "API Rate Limiting Middleware"
    ```go title="api_middleware.go" linums="1" hl_lines="8-150"
    package main

    import (
        "fmt"
        "net/http"
        "sync"
        "time"
        "golang.org/x/time/rate"
    )

    // RateLimiterConfig holds configuration for rate limiting
    type RateLimiterConfig struct {
        RequestsPerSecond int
        BurstSize        int
        CleanupInterval  time.Duration
    }

    // ClientRateLimiter tracks rate limiting per client
    type ClientRateLimiter struct {
        limiter  *rate.Limiter
        lastSeen time.Time
    }

    // RateLimitMiddleware implements rate limiting middleware
    type RateLimitMiddleware struct {
        limiters map[string]*ClientRateLimiter
        mu       sync.RWMutex
        config   RateLimiterConfig
    }

    // NewRateLimitMiddleware creates a new rate limiting middleware
    func NewRateLimitMiddleware(config RateLimiterConfig) *RateLimitMiddleware {
        m := &RateLimitMiddleware{
            limiters: make(map[string]*ClientRateLimiter),
            config:   config,
        }
        
        // Start cleanup goroutine
        go m.cleanupStaleLimiters()
        
        return m
    }

    // cleanupStaleLimiters removes limiters that haven't been used recently
    func (m *RateLimitMiddleware) cleanupStaleLimiters() {
        ticker := time.NewTicker(m.config.CleanupInterval)
        defer ticker.Stop()
        
        for range ticker.C {
            m.mu.Lock()
            for ip, limiter := range m.limiters {
                if time.Since(limiter.lastSeen) > m.config.CleanupInterval {
                    delete(m.limiters, ip)
                }
            }
            m.mu.Unlock()
        }
    }

    // getLimiter returns a rate limiter for the given IP
    func (m *RateLimitMiddleware) getLimiter(ip string) *rate.Limiter {
        m.mu.Lock()
        defer m.mu.Unlock()
        
        limiter, exists := m.limiters[ip]
        if !exists {
            limiter = &ClientRateLimiter{
                limiter: rate.NewLimiter(
                    rate.Limit(m.config.RequestsPerSecond),
                    m.config.BurstSize,
                ),
                lastSeen: time.Now(),
            }
            m.limiters[ip] = limiter
        }
        
        limiter.lastSeen = time.Now()
        return limiter.limiter
    }

    // Middleware returns the HTTP middleware handler
    func (m *RateLimitMiddleware) Middleware(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            // Get client IP (simplified for example)
            ip := r.RemoteAddr
            
            // Get rate limiter for this IP
            limiter := m.getLimiter(ip)
            
            // Check if request is allowed
            if !limiter.Allow() {
                http.Error(w, "Rate limit exceeded", http.StatusTooManyRequests)
                return
            }
            
            // Add rate limit headers
            w.Header().Set("X-RateLimit-Limit", fmt.Sprintf("%d", m.config.RequestsPerSecond))
            w.Header().Set("X-RateLimit-Remaining", fmt.Sprintf("%d", limiter.Tokens()))
            w.Header().Set("X-RateLimit-Reset", fmt.Sprintf("%d", time.Now().Add(time.Second).Unix()))
            
            // Call next handler
            next.ServeHTTP(w, r)
        })
    }

    // APIHandler is a simple API endpoint handler
    func APIHandler(w http.ResponseWriter, r *http.Request) {
        fmt.Fprintf(w, "API request processed successfully")
    }

    func main() {
        // Create rate limiting middleware
        config := RateLimiterConfig{
            RequestsPerSecond: 10,
            BurstSize:        20,
            CleanupInterval:  time.Hour,
        }
        
        rateLimitMiddleware := NewRateLimitMiddleware(config)
        
        // Create router
        mux := http.NewServeMux()
        
        // Apply middleware to API endpoint
        mux.Handle("/api", rateLimitMiddleware.Middleware(http.HandlerFunc(APIHandler)))
        
        // Start server
        fmt.Println("Server starting on :8080")
        http.ListenAndServe(":8080", mux)
    }
    ```

### How This Example Demonstrates Rate Limiting Concepts:

1. **Token Bucket Implementation**:
   - Uses Go's `x/time/rate` package for production-ready token bucket
   - Configurable requests per second and burst size

2. **Per-Client Rate Limiting**:
   - Tracks rate limiters separately for each client IP
   - Prevents one client from affecting others

3. **Resource Management**:
   - Cleanup goroutine removes stale limiters to prevent memory leaks
   - Thread-safe access to limiters map

4. **HTTP Integration**:
   - Implements standard HTTP middleware pattern
   - Returns appropriate HTTP status codes (429 for rate limit exceeded)
   - Includes rate limit headers for client information

5. **Configurable Parameters**:
   - Easy to adjust rate limits based on requirements
   - Separate configuration for burst size and sustained rate

## Quick Reference

!!! success "Key Takeaways"
    - **Token Bucket**: Best for allowing controlled bursts, refills at fixed rate, ideal for APIs
    - **Fixed Window Counter**: Simple implementation, but vulnerable to boundary bursts
    - **Leaky Bucket**: Smooths traffic, prevents bursts, good for steady processing
    - **Go's x/time/rate**: Production-ready Token Bucket implementation in standard library
    - **Distributed Rate Limiting**: Use Redis or similar for coordination across multiple instances
    - **Algorithm Selection**: Choose based on burst tolerance, implementation complexity, and traffic patterns
    - **Multi-tier Limiting**: Implement different limits for different user tiers or endpoints
    - **Dynamic Adjustment**: Monitor system load and adjust limits dynamically
    - **HTTP Integration**: Implement as middleware with proper status codes and headers
    - **Resource Management**: Clean up stale limiters to prevent memory leaks

!!! quote "Remember"
    "Rate limiting is a critical component of any robust service, protecting against abuse, managing resources, and ensuring fair usage. The choice of algorithm depends on your specific requirements: Token Bucket for controlled bursts, Fixed Window for simplicity, and Leaky Bucket for traffic smoothing. By implementing proper rate limiting, you can build more resilient and user-friendly services that handle traffic spikes gracefully while maintaining system stability."}