# Worker Pools, Wait Groups, Mutexes, and Atomic Counters in Go: A Comprehensive Guide

!!! abstract "Overview"
    Master Go's essential concurrency primitives for building robust and efficient concurrent applications. Learn to implement worker pools for parallel processing, synchronize goroutines with WaitGroups, protect shared data with Mutexes, and perform lock-free operations with Atomic Counters. Understand when and how to use each primitive, their performance characteristics, and how they form the foundation of Go's concurrency model.

!!! tip "Key Points"
    - **Worker Pools**: Manage a fixed number of goroutines to process tasks from a queue, optimizing resource usage and preventing goroutine explosion
    - **WaitGroups**: Synchronize multiple goroutines by waiting for a collection of goroutines to finish executing
    - **Mutexes**: Provide mutual exclusion to protect shared data from race conditions, ensuring only one goroutine accesses critical sections at a time
    - **Atomic Counters**: Perform lock-free operations on simple numeric types using atomic instructions, avoiding mutex overhead for simple operations
    - **Trade-offs**: Each primitive has specific use cases - worker pools for task distribution, WaitGroups for synchronization, Mutexes for complex data protection, and atomics for simple counters
    - **Performance Considerations**: Choose the right primitive based on contention levels, operation complexity, and performance requirements
    - **Best Practices**: Always handle errors properly, avoid deadlocks, and prefer higher-level abstractions when possible

## Worker Pools

Worker pools are a concurrency pattern that uses a fixed number of worker goroutines to process tasks from a queue. This pattern helps control resource usage and prevents the creation of too many goroutines.

!!! info "Worker Pool Architecture"
    ```mermaid
    graph TB
        A[Task Queue] --> B[Worker 1]
        A --> C[Worker 2]
        A --> D[Worker 3]
        A --> E[Worker N]
        B --> F[Result Collector]
        C --> F
        D --> F
        E --> F
        style A fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style F fill:#9cf,stroke:#333,stroke-width:2px,color:#000
    ```

### Basic Worker Pool Implementation

!!! example "Basic Worker Pool"
    ```go title="worker_pool.go" linenums="1" hl_lines="8-54"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    // Worker represents a worker goroutine
    type Worker struct {
        ID         int
        TaskQueue  chan Task
        ResultChan chan Result
        QuitChan   chan struct{}
    }

    // Task represents a unit of work
    type Task struct {
        ID   int
        Data string
    }

    // Result represents the result of a task
    type Result struct {
        TaskID int
        Output string
        Error  error
    }

    // NewWorker creates a new worker
    func NewWorker(id int, taskQueue chan Task, resultChan chan Result) *Worker {
        return &Worker{
            ID:         id,
            TaskQueue:  taskQueue,
            ResultChan: resultChan,
            QuitChan:   make(chan struct{}),
        }
    }

    // Start begins the worker's processing loop
    func (w *Worker) Start() {
        go func() {
            for {
                select {
                case task := <-w.TaskQueue:
                    // Process the task
                    result := w.processTask(task)
                    w.ResultChan <- result
                case <-w.QuitChan:
                    fmt.Printf("Worker %d shutting down\n", w.ID)
                    return
                }
            }
        }()
    }

    // processTask simulates processing a task
    func (w *Worker) processTask(task Task) Result {
        fmt.Printf("Worker %d processing task %d\n", w.ID, task.ID)
        time.Sleep(time.Duration(task.ID%3+1) * 100 * time.Millisecond) // Simulate work
        return Result{
            TaskID: task.ID,
            Output: fmt.Sprintf("Processed: %s", task.Data),
        }
    }

    // Stop signals the worker to stop
    func (w *Worker) Stop() {
        close(w.QuitChan)
    }

    // WorkerPool manages a pool of workers
    type WorkerPool struct {
        Workers    []*Worker
        TaskQueue  chan Task
        ResultChan chan Result
        QuitChan   chan struct{}
    }

    // NewWorkerPool creates a new worker pool
    func NewWorkerPool(numWorkers int) *WorkerPool {
        taskQueue := make(chan Task, 100)
        resultChan := make(chan Result, 100)
        quitChan := make(chan struct{})
        
        workers := make([]*Worker, numWorkers)
        for i := 0; i < numWorkers; i++ {
            workers[i] = NewWorker(i+1, taskQueue, resultChan)
            workers[i].Start()
        }
        
        return &WorkerPool{
            Workers:    workers,
            TaskQueue:  taskQueue,
            ResultChan: resultChan,
            QuitChan:   quitChan,
        }
    }

    // Submit adds a task to the pool
    func (wp *WorkerPool) Submit(task Task) {
        wp.TaskQueue <- task
    }

    // Results returns the result channel
    func (wp *WorkerPool) Results() <-chan Result {
        return wp.ResultChan
    }

    // Stop shuts down all workers
    func (wp *WorkerPool) Stop() {
        close(wp.QuitChan)
        for _, worker := range wp.Workers {
            worker.Stop()
        }
        close(wp.TaskQueue)
        close(wp.ResultChan)
    }

    func main() {
        // Create a worker pool with 3 workers
        pool := NewWorkerPool(3)
        defer pool.Stop()
        
        // Submit 10 tasks
        for i := 1; i <= 10; i++ {
            task := Task{
                ID:   i,
                Data: fmt.Sprintf("Task-%d", i),
            }
            pool.Submit(task)
        }
        
        // Collect results
        completed := 0
        for result := range pool.Results() {
            fmt.Printf("Task %d completed: %s\n", result.TaskID, result.Output)
            completed++
            if completed >= 10 {
                break
            }
        }
        
        fmt.Println("All tasks completed")
    }
    ```

### Dynamic Worker Pool with Scaling

!!! example "Dynamic Worker Pool"
    ```go title="dynamic_worker_pool.go" linenums="1" hl_lines="8-80"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    type Task struct {
        ID   int
        Data string
    }

    type Result struct {
        TaskID int
        Output string
    }

    type DynamicWorkerPool struct {
        minWorkers    int
        maxWorkers    int
        currentWorkers int
        taskQueue     chan Task
        resultChan    chan Result
        workerWg      sync.WaitGroup
        mu            sync.Mutex
        quitChan      chan struct{}
    }

    func NewDynamicWorkerPool(min, max int) *DynamicWorkerPool {
        if min < 1 {
            min = 1
        }
        if max < min {
            max = min
        }
        
        pool := &DynamicWorkerPool{
            minWorkers:    min,
            maxWorkers:    max,
            currentWorkers: 0,
            taskQueue:     make(chan Task, 100),
            resultChan:    make(chan Result, 100),
            quitChan:      make(chan struct{}),
        }
        
        // Start initial workers
        for i := 0; i < min; i++ {
            pool.addWorker()
        }
        
        // Start scaler
        go pool.scaler()
        
        return pool
    }

    func (p *DynamicWorkerPool) addWorker() {
        p.mu.Lock()
        defer p.mu.Unlock()
        
        if p.currentWorkers >= p.maxWorkers {
            return
        }
        
        p.workerWg.Add(1)
        p.currentWorkers++
        
        go func(workerID int) {
            defer p.workerWg.Done()
            
            for {
                select {
                case task := <-p.taskQueue:
                    // Process task
                    time.Sleep(time.Duration(task.ID%3+1) * 100 * time.Millisecond)
                    p.resultChan <- Result{
                        TaskID: task.ID,
                        Output: fmt.Sprintf("Worker %d processed: %s", workerID, task.Data),
                    }
                case <-p.quitChan:
                    fmt.Printf("Worker %d shutting down\n", workerID)
                    return
                }
            }
        }(p.currentWorkers)
    }

    func (p *DynamicWorkerPool) removeWorker() {
        p.mu.Lock()
        defer p.mu.Unlock()
        
        if p.currentWorkers <= p.minWorkers {
            return
        }
        
        // Signal one worker to quit
        p.quitChan <- struct{}{}
        p.currentWorkers--
    }

    func (p *DynamicWorkerPool) scaler() {
       	ticker := time.NewTicker(1 * time.Second)
        defer ticker.Stop()
        
        for {
            select {
            case <-ticker.C:
                queueLen := len(p.taskQueue)
                p.mu.Lock()
               	currentWorkers := p.currentWorkers
                p.mu.Unlock()
                
                // Scale up if queue is long and we can add more workers
                if queueLen > currentWorkers*2 && currentWorkers < p.maxWorkers {
                    fmt.Printf("Scaling up: %d workers -> %d workers\n", currentWorkers, currentWorkers+1)
                    p.addWorker()
				}
                // Scale down if queue is empty and we have more than minimum workers
                else if queueLen == 0 && currentWorkers > p.minWorkers {
                    fmt.Printf("Scaling down: %d workers -> %d workers\n", currentWorkers, currentWorkers-1)
                    p.removeWorker()
				}
            case <-p.quitChan:
                return
            }
        }
    }

    func (p *DynamicWorkerPool) Submit(task Task) {
        p.taskQueue <- task
    }

    func (p *DynamicWorkerPool) Results() <-chan Result {
        return p.resultChan
    }

    func (p *DynamicWorkerPool) Stop() {
        close(p.quitChan)
        close(p.taskQueue)
        p.workerWg.Wait()
        close(p.resultChan)
    }

    func main() {
        pool := NewDynamicWorkerPool(2, 5)
        defer pool.Stop()
        
        // Submit tasks in bursts
        for burst := 0; burst < 3; burst++ {
            fmt.Printf("Submitting burst %d\n", burst+1)
            for i := 1; i <= 5; i++ {
                taskID := burst*5 + i
                pool.Submit(Task{
                    ID:   taskID,
                    Data: fmt.Sprintf("Task-%d", taskID),
                })
            }
            time.Sleep(2 * time.Second)
        }
        
        // Collect results
        completed := 0
        for result := range pool.Results() {
            fmt.Printf("%s\n", result.Output)
            completed++
            if completed >= 15 {
                break
            }
        }
    }
    ```

## Wait Groups

WaitGroups are a synchronization primitive that allows you to wait for a collection of goroutines to finish executing. They're essential when you need to coordinate multiple goroutines and wait for their completion.

!!! info "WaitGroup Mechanism"
    ```mermaid
    graph TB
        A[Main Goroutine] --> B[WaitGroup]
        B --> C[Goroutine 1]
        B --> D[Goroutine 2]
        B --> E[Goroutine 3]
        C --> F[Done]
        D --> F
        E --> F
        F --> G[Wait Completes]
        G --> A
        style A fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style B fill:#f9f,stroke:#333,stroke-width:2px,color:#000
    ```

### Basic WaitGroup Usage

!!! example "Basic WaitGroup"
    ```go title="basic_waitgroup.go" linenums="1" hl_lines="8-32"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    func worker(id int, wg *sync.WaitGroup) {
        defer wg.Done() // Decrement counter when done
        
        fmt.Printf("Worker %d starting\n", id)
        time.Sleep(time.Duration(id) * 100 * time.Millisecond)
        fmt.Printf("Worker %d done\n", id)
    }

    func main() {
        var wg sync.WaitGroup
        
        // Start 3 workers
        for i := 1; i <= 3; i++ {
            wg.Add(1) // Increment counter for each worker
            go worker(i, &wg)
        }
        
        // Wait for all workers to finish
        wg.Wait()
        fmt.Println("All workers completed")
    }
    ```

### WaitGroup with Error Handling

!!! example "WaitGroup with Error Handling"
    ```go title="waitgroup_errors.go" linenums="1" hl_lines="8-50"
    package main

    import (
        "errors"
        "fmt"
        "sync"
        "time"
    )

    func worker(id int, wg *sync.WaitGroup, errChan chan<- error) {
        defer wg.Done()
        
        // Simulate work with occasional errors
        time.Sleep(time.Duration(id) * 100 * time.Millisecond)
        
        if id%3 == 0 {
            errChan <- fmt.Errorf("worker %d failed", id)
            return
        }
        
        fmt.Printf("Worker %d completed successfully\n", id)
    }

    func main() {
        var wg sync.WaitGroup
        errChan := make(chan error, 10)
        
        // Start 10 workers
        for i := 1; i <= 10; i++ {
            wg.Add(1)
            go worker(i, &wg, errChan)
        }
        
        // Start a goroutine to close errChan when all workers are done
        go func() {
            wg.Wait()
            close(errChan)
        }()
        
        // Collect errors
        var errs []error
        for err := range errChan {
            errs = append(errs, err)
        }
        
        // Report results
        if len(errs) > 0 {
            fmt.Printf("Completed with %d errors:\n", len(errs))
            for _, err := range errs {
                fmt.Printf("- %v\n", err)
            }
        } else {
            fmt.Println("All workers completed successfully")
        }
    }
    ```

### Nested WaitGroups

!!! example "Nested WaitGroups"
    ```go title="nested_waitgroups.go" linenums="1" hl_lines="8-52"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    func subWorker(id int, parentWg *sync.WaitGroup) {
        defer parentWg.Done()
        
        var subWg sync.WaitGroup
        
        // Start 3 sub-workers
        for i := 1; i <= 3; i++ {
            subWg.Add(1)
            go func(subID int) {
                defer subWg.Done()
                fmt.Printf("Sub-worker %d.%d starting\n", id, subID)
                time.Sleep(100 * time.Millisecond)
                fmt.Printf("Sub-worker %d.%d done\n", id, subID)
            }(i)
        }
        
        // Wait for all sub-workers to complete
        subWg.Wait()
        fmt.Printf("Worker %d: all sub-workers completed\n", id)
    }

    func main() {
        var wg sync.WaitGroup
        
        // Start 3 main workers
        for i := 1; i <= 3; i++ {
            wg.Add(1)
            go subWorker(i, &wg)
        }
        
        // Wait for all main workers and their sub-workers to complete
        wg.Wait()
        fmt.Println("All workers and sub-workers completed")
    }
    ```

## Mutexes

Mutexes (mutual exclusion locks) are used to protect shared resources from being accessed concurrently by multiple goroutines. They ensure that only one goroutine can access the critical section at a time.

!!! info "Mutex Mechanism"
    ```mermaid
    graph TB
        A[Goroutine 1] -->|Lock| B[Mutex]
        C[Goroutine 2] -->|Wait| B
        D[Goroutine 3] -->|Wait| B
        B -->|Unlock| A
        B -->|Lock| C
        style A fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style C fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style B fill:#f9f,stroke:#333,stroke-width:2px,color:#000
    ```

### Basic Mutex Usage

!!! example "Basic Mutex"
    ```go title="basic_mutex.go" linenums="1" hl_lines="8-36"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    type Counter struct {
        mu    sync.Mutex
        value int
    }

    func (c *Counter) Increment() {
        c.mu.Lock()
        defer c.mu.Unlock()
        c.value++
    }

    func (c *Counter) Value() int {
        c.mu.Lock()
        defer c.mu.Unlock()
        return c.value
    }

    func main() {
        counter := Counter{}
        var wg sync.WaitGroup
        
        // Start 1000 goroutines to increment the counter
        for i := 0; i < 1000; i++ {
            wg.Add(1)
            go func() {
                defer wg.Done()
                counter.Increment()
            }()
        }
        
        wg.Wait()
        fmt.Println("Final counter value:", counter.Value())
    }
    ```

### RWMutex for Read-Write Locking

!!! example "RWMutex Usage"
    ```go title="rwmutex.go" linenums="1" hl_lines="8-60"
    package main

    import (
        "fmt"
        "math/rand"
        "sync"
        "time"
    )

    type DataStore struct {
        mu    sync.RWMutex
        data  map[string]int
    }

    func NewDataStore() *DataStore {
        return &DataStore{
            data: make(map[string]int),
        }
    }

    func (ds *DataStore) Read(key string) (int, bool) {
        ds.mu.RLock()
        defer ds.mu.RUnlock()
        value, exists := ds.data[key]
        return value, exists
    }

    func (ds *DataStore) Write(key string, value int) {
        ds.mu.Lock()
        defer ds.mu.Unlock()
        ds.data[key] = value
    }

    func main() {
        store := NewDataStore()
        var wg sync.WaitGroup
        
        // Initialize with some data
        store.Write("apple", 5)
        store.Write("banana", 3)
        store.Write("orange", 8)
        
        // Start readers
        for i := 0; i < 10; i++ {
            wg.Add(1)
            go func(id int) {
                defer wg.Done()
                for j := 0; j < 5; j++ {
                    key := []string{"apple", "banana", "orange"}[rand.Intn(3)]
                    value, exists := store.Read(key)
                    if exists {
                        fmt.Printf("Reader %d: %s = %d\n", id, key, value)
                    }
                    time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
                }
            }(i)
        }
        
        // Start writers
        for i := 0; i < 3; i++ {
            wg.Add(1)
            go func(id int) {
                defer wg.Done()
                for j := 0; j < 3; j++ {
                    key := []string{"apple", "banana", "orange"}[rand.Intn(3)]
                    value := rand.Intn(10)
                    store.Write(key, value)
                    fmt.Printf("Writer %d: set %s = %d\n", id, key, value)
                    time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)
                }
            }(i)
        }
        
        wg.Wait()
        fmt.Println("All operations completed")
    }
    ```

### Mutex with Timeout

!!! example "Mutex with Timeout"
    ```go title="mutex_timeout.go" linenums="1" hl_lines="8-46"
    package main

    import (
        "fmt"
        "sync"
        "time"
    )

    type TimedCounter struct {
        mu    sync.Mutex
        value int
    }

    func (c *TimedCounter) IncrementWithTimeout(timeout time.Duration) bool {
        // Try to acquire lock with timeout
        acquired := make(chan struct{})
        go func() {
            c.mu.Lock()
            close(acquired)
        }()
        
        select {
        case <-acquired:
            defer c.mu.Unlock()
            c.value++
            return true
        case <-time.After(timeout):
            return false
        }
    }

    func (c *TimedCounter) Value() int {
        c.mu.Lock()
        defer c.mu.Unlock()
        return c.value
    }

    func main() {
        counter := TimedCounter{}
        
        // Lock the counter
        counter.mu.Lock()
        
        // Try to increment with timeout in a goroutine
        go func() {
            success := counter.IncrementWithTimeout(100 * time.Millisecond)
            if success {
                fmt.Println("Increment succeeded")
            } else {
                fmt.Println("Increment timed out")
            }
        }()
        
        // Sleep to ensure the timeout occurs
        time.Sleep(200 * time.Millisecond)
        counter.mu.Unlock()
        
        fmt.Println("Final counter value:", counter.Value())
    }
    ```

## Atomic Counters

Atomic operations provide lock-free synchronization for simple operations on numeric types. They're more efficient than mutexes for simple operations like incrementing counters.

!!! info "Atomic Operations"
    ```mermaid
    graph LR
        A[Goroutine 1] -->|Atomic Operation| B[Memory]
        C[Goroutine 2] -->|Atomic Operation| B
        D[Goroutine 3] -->|Atomic Operation| B
        B -->|Hardware Guarantee| E[Consistent State]
        style A fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style C fill:#ccf,stroke:#333,stroke-width:2px,color:#000
        style D fill:#ccf,stroke:#333,stroke-width:2px,color:#000
    ```

### Basic Atomic Operations

!!! example "Basic Atomic Operations"
    ```go title="basic_atomic.go" linenums="1" hl_lines="8-42"
    package main

    import (
        "fmt"
        "sync"
        "sync/atomic"
    )

    type AtomicCounter struct {
        value int64
    }

    func (c *AtomicCounter) Increment() {
        atomic.AddInt64(&c.value, 1)
    }

    func (c *AtomicCounter) Decrement() {
        atomic.AddInt64(&c.value, -1)
    }

    func (c *AtomicCounter) Get() int64 {
        return atomic.LoadInt64(&c.value)
    }

    func (c *AtomicCounter) Set(newValue int64) {
        atomic.StoreInt64(&c.value, newValue)
    }

    func (c *AtomicCounter) CompareAndSwap(oldValue, newValue int64) bool {
        return atomic.CompareAndSwapInt64(&c.value, oldValue, newValue)
    }

    func main() {
        counter := AtomicCounter{}
        var wg sync.WaitGroup
        
        // Start 1000 goroutines to increment the counter
        for i := 0; i < 1000; i++ {
            wg.Add(1)
            go func() {
                defer wg.Done()
                counter.Increment()
            }()
        }
        
        wg.Wait()
        fmt.Println("Final counter value:", counter.Get())
        
        // Demonstrate CompareAndSwap
        old := counter.Get()
        success := counter.CompareAndSwap(old, old+100)
        fmt.Printf("CompareAndSwap from %d to %d: %v\n", old, old+100, success)
        fmt.Println("New value:", counter.Get())
    }
    ```

### Atomic Pointers

!!! example "Atomic Pointers"
    ```go title="atomic_pointers.go" linenums="1" hl_lines="8-50"
    package main

    import (
        "fmt"
        "sync"
        "sync/atomic"
        "time"
    )

    type Config struct {
        MaxConnections int
        Timeout        time.Duration
    }

    type AtomicConfig struct {
        pointer atomic.Value
    }

    func (ac *AtomicConfig) Load() *Config {
        return ac.pointer.Load().(*Config)
    }

    func (ac *AtomicConfig) Store(config *Config) {
        ac.pointer.Store(config)
    }

    func worker(id int, config *AtomicConfig, wg *sync.WaitGroup) {
        defer wg.Done()
        
        for i := 0; i < 5; i++ {
            cfg := config.Load()
            fmt.Printf("Worker %d: MaxConnections=%d, Timeout=%v\n", 
                id, cfg.MaxConnections, cfg.Timeout)
            time.Sleep(100 * time.Millisecond)
        }
    }

    func main() {
        config := &AtomicConfig{}
        
        // Initialize with default config
        config.Store(&Config{
            MaxConnections: 10,
            Timeout:        5 * time.Second,
        })
        
        var wg sync.WaitGroup
        
        // Start workers
        for i := 1; i <= 3; i++ {
            wg.Add(1)
            go worker(i, config, &wg)
        }
        
        // Update config after some time
        go func() {
            time.Sleep(250 * time.Millisecond)
            config.Store(&Config{
                MaxConnections: 20,
                Timeout:        10 * time.Second,
            })
            fmt.Println("Config updated")
        }()
        
        wg.Wait()
        fmt.Println("All workers completed")
    }
    ```

### Atomic with Custom Types

!!! example "Atomic with Custom Types"
    ```go title="atomic_custom.go" linenums="1" hl_lines="8-60"
    package main

    import (
        "fmt"
        "sync"
        "sync/atomic"
        "time"
    )

    type Stats struct {
        Requests  int64
        Errors    int64
        Duration  time.Duration
    }

    type AtomicStats struct {
        stats atomic.Value
    }

    func NewAtomicStats() *AtomicStats {
        stats := &AtomicStats{}
        stats.stats.Store(&Stats{})
        return stats
    }

    func (as *AtomicStats) AddRequest() {
        for {
            old := as.stats.Load().(*Stats)
            new := &Stats{
                Requests: old.Requests + 1,
                Errors:   old.Errors,
                Duration: old.Duration,
            }
            if as.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (as *AtomicStats) AddError() {
        for {
            old := as.stats.Load().(*Stats)
            new := &Stats{
                Requests: old.Requests,
                Errors:   old.Errors + 1,
                Duration: old.Duration,
            }
            if as.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (as *AtomicStats) AddDuration(d time.Duration) {
        for {
            old := as.stats.Load().(*Stats)
            new := &Stats{
                Requests: old.Requests,
                Errors:   old.Errors,
                Duration: old.Duration + d,
            }
            if as.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (as *AtomicStats) Get() Stats {
        return *as.stats.Load().(*Stats)
    }

    func main() {
        stats := NewAtomicStats()
        var wg sync.WaitGroup
        
        // Simulate requests
        for i := 0; i < 100; i++ {
            wg.Add(1)
            go func(id int) {
                defer wg.Done()
                
                start := time.Now()
                stats.AddRequest()
                
                // Simulate work
                time.Sleep(time.Duration(id%10) * 10 * time.Millisecond)
                
                // Simulate occasional errors
                if id%7 == 0 {
                    stats.AddError()
                }
                
                stats.AddDuration(time.Since(start))
            }(i)
        }
        
        wg.Wait()
        
        // Print final stats
        final := stats.Get()
        fmt.Printf("Requests: %d\n", final.Requests)
        fmt.Printf("Errors: %d\n", final.Errors)
        fmt.Printf("Total Duration: %v\n", final.Duration)
        fmt.Printf("Average Duration: %v\n", final.Duration/time.Duration(final.Requests))
    }
    ```

## Best Practices

!!! tip "Worker Pool Best Practices"
    - Choose an appropriate pool size based on the nature of tasks (CPU-bound vs I/O-bound)
    - Use buffered channels for task queues to prevent blocking
    - Implement graceful shutdown with quit channels
    - Handle task errors properly and propagate them to the caller
    - Monitor queue length to adjust pool size dynamically if needed

!!! example "Worker Pool with Error Handling"
    ```go title="worker_pool_errors.go" linenum="1" hl_lines="8-30"
    package main

    import (
        "fmt"
        "math/rand"
        "sync"
        "time"
    )

    type Task struct {
        ID int
    }

    type Result struct {
        TaskID int
        Error  error
    }

    func worker(id int, tasks <-chan Task, results chan<- Result) {
        for task := range tasks {
            // Simulate work with occasional errors
            time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
            
            var err error
            if rand.Intn(10) == 0 {
                err = fmt.Errorf("worker %d failed on task %d", id, task.ID)
            }
            
            results <- Result{TaskID: task.ID, Error: err}
        }
    }

    func main() {
        tasks := make(chan Task, 100)
        results := make(chan Result, 100)
        
        // Start 3 workers
        var wg sync.WaitGroup
        for i := 1; i <= 3; i++ {
            wg.Add(1)
            go func(id int) {
                defer wg.Done()
                worker(id, tasks, results)
            }(i)
        }
        
        // Close results channel when workers are done
        go func() {
            wg.Wait()
            close(results)
        }()
        
        // Submit tasks
        for i := 1; i <= 20; i++ {
            tasks <- Task{ID: i}
        }
        close(tasks)
        
        // Collect results
        for result := range results {
            if result.Error != nil {
                fmt.Printf("Task %d failed: %v\n", result.TaskID, result.Error)
            } else {
                fmt.Printf("Task %d completed successfully\n", result.TaskID)
            }
        }
    }
    ```

!!! tip "WaitGroup Best Practices"
    - Always call `Add()` before starting the goroutine
    - Use `defer wg.Done()` to ensure the counter is decremented even if panic occurs
    - Never copy a WaitGroup after first use
    - Be careful with nested WaitGroups to avoid deadlocks
    - Consider using error channels to collect errors from goroutines

!!! tip "Mutex Best Practices"
    - Always unlock mutexes, preferably with `defer`
    - Keep critical sections as short as possible
    - Avoid calling unknown functions while holding a lock
    - Be aware of potential deadlocks when using multiple mutexes
    - Use RWMutex when reads significantly outnumber writes
    - Consider using sync.Once for one-time initialization

!!! tip "Atomic Best Practices"
    - Use atomics for simple operations on primitive types
    - Prefer atomic operations over mutexes for performance-critical counters
    - Use atomic.Value for complex types that are occasionally updated
    - Be careful with atomic operations on pointers - ensure proper memory management
    - Remember that atomic operations don't provide the same visibility guarantees as mutexes for complex operations

## Real-World Example: Concurrent Web Server

Let's create a comprehensive example that combines all these concurrency primitives:

!!! example "Concurrent Web Server"
    ```go title="web_server.go" linenum="1" hl_lines="8-150"
    package main

    import (
        "fmt"
        "log"
        "net/http"
        "sync"
        "sync/atomic"
        "time"
    )

    // ServerStats tracks server statistics
    type ServerStats struct {
        Requests     int64
        ActiveConns  int32
        Errors       int64
        TotalLatency int64
    }

    // AtomicServerStats provides atomic access to server statistics
    type AtomicServerStats struct {
        stats atomic.Value
    }

    func NewAtomicServerStats() *AtomicServerStats {
        stats := &AtomicServerStats{}
        stats.stats.Store(&ServerStats{})
        return stats
    }

    func (ass *AtomicServerStats) incrementRequests() {
        for {
            old := ass.stats.Load().(*ServerStats)
            new := &ServerStats{
                Requests:     old.Requests + 1,
                ActiveConns:  old.ActiveConns,
                Errors:       old.Errors,
                TotalLatency: old.TotalLatency,
            }
            if ass.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (ass *AtomicServerStats) incrementActiveConns() {
        for {
            old := ass.stats.Load().(*ServerStats)
            new := &ServerStats{
                Requests:     old.Requests,
                ActiveConns:  old.ActiveConns + 1,
                Errors:       old.Errors,
                TotalLatency: old.TotalLatency,
            }
            if ass.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (ass *AtomicServerStats) decrementActiveConns() {
        for {
            old := ass.stats.Load().(*ServerStats)
            new := &ServerStats{
                Requests:     old.Requests,
                ActiveConns:  old.ActiveConns - 1,
                Errors:       old.Errors,
                TotalLatency: old.TotalLatency,
            }
            if ass.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (ass *AtomicServerStats) incrementErrors() {
        for {
            old := ass.stats.Load().(*ServerStats)
            new := &ServerStats{
                Requests:     old.Requests,
                ActiveConns:  old.ActiveConns,
                Errors:       old.Errors + 1,
                TotalLatency: old.TotalLatency,
            }
            if ass.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (ass *AtomicServerStats) addLatency(d time.Duration) {
        for {
            old := ass.stats.Load().(*ServerStats)
            new := &ServerStats{
                Requests:     old.Requests,
                ActiveConns:  old.ActiveConns,
                Errors:       old.Errors,
                TotalLatency: old.TotalLatency + d.Nanoseconds(),
            }
            if ass.stats.CompareAndSwap(old, new) {
                break
            }
        }
    }

    func (ass *AtomicServerStats) Get() ServerStats {
        return *ass.stats.Load().(*ServerStats)
    }

    // Request represents a web request
    type Request struct {
        ID        int
        Path      string
        StartTime time.Time
    }

    // Response represents a web response
    type Response struct {
        RequestID int
        Status    int
        Latency   time.Duration
        Error     error
    }

    // Worker represents a request handler worker
    type Worker struct {
        ID       int
        Requests <-chan Request
        Responses chan<- Response
        Quit     chan struct{}
        Stats    *AtomicServerStats
    }

    func (w *Worker) Start() {
        go func() {
            for {
                select {
                case req := <-w.Requests:
                    w.Stats.incrementActiveConns()
                    start := time.Now()
                    
                    // Simulate request processing
                    time.Sleep(time.Duration(10+req.ID%50) * time.Millisecond)
                    
                    latency := time.Since(start)
                    w.Stats.addLatency(latency)
                    w.Stats.decrementActiveConns()
                    
                    // Simulate occasional errors
                    var err error
                    status := http.StatusOK
                    if req.ID%10 == 0 {
                        err = fmt.Errorf("internal server error")
                        status = http.StatusInternalServerError
                        w.Stats.incrementErrors()
                    }
                    
                    w.Responses <- Response{
                        RequestID: req.ID,
                        Status:    status,
                        Latency:   latency,
                        Error:     err,
                    }
                    
                case <-w.Quit:
                    return
                }
            }
        }()
    }

    // WebServer represents a concurrent web server
    type WebServer struct {
        Workers    []*Worker
        RequestCh  chan Request
        ResponseCh chan Response
        Stats      *AtomicServerStats
        WorkerWg   sync.WaitGroup
        mu         sync.Mutex
        quit       chan struct{}
    }

    func NewWebServer(numWorkers int) *WebServer {
        requestCh := make(chan Request, 100)
        responseCh := make(chan Response, 100)
        stats := NewAtomicServerStats()
        
        workers := make([]*Worker, numWorkers)
        for i := 0; i < numWorkers; i++ {
            workers[i] = &Worker{
                ID:       i + 1,
                Requests: requestCh,
                Responses: responseCh,
                Quit:     make(chan struct{}),
                Stats:    stats,
            }
            workers[i].Start()
        }
        
        return &WebServer{
            Workers:    workers,
            RequestCh:  requestCh,
            ResponseCh: responseCh,
            Stats:      stats,
            quit:       make(chan struct{}),
        }
    }

    func (ws *WebServer) HandleRequest(req Request) {
        ws.Stats.incrementRequests()
        ws.RequestCh <- req
    }

    func (ws *WebServer) StartResponseCollector() {
        go func() {
            for resp := range ws.ResponseCh {
                if resp.Error != nil {
                    log.Printf("Request %d failed: %v (latency: %v)", 
                        resp.RequestID, resp.Error, resp.Latency)
                } else {
                    log.Printf("Request %d completed with status %d (latency: %v)", 
                        resp.RequestID, resp.Status, resp.Latency)
                }
            }
        }()
    }

    func (ws *WebServer) Stop() {
        close(ws.quit)
        for _, worker := range ws.Workers {
            close(worker.Quit)
        }
        close(ws.RequestCh)
        close(ws.ResponseCh)
    }

    func (ws *WebServer) GetStats() ServerStats {
        return ws.Stats.Get()
    }

    func main() {
        // Create web server with 5 workers
        server := NewWebServer(5)
        server.StartResponseCollector()
        defer server.Stop()
        
        // Simulate incoming requests
        var wg sync.WaitGroup
        for i := 1; i <= 100; i++ {
            wg.Add(1)
            go func(id int) {
                defer wg.Done()
                server.HandleRequest(Request{
                    ID:        id,
                    Path:      fmt.Sprintf("/api/resource/%d", id),
                    StartTime: time.Now(),
                })
            }(i)
        }
        
        wg.Wait()
        
        // Print final statistics
        stats := server.GetStats()
        fmt.Printf("\nFinal Statistics:\n")
        fmt.Printf("Total Requests: %d\n", stats.Requests)
        fmt.Printf("Active Connections: %d\n", stats.ActiveConns)
        fmt.Printf("Errors: %d\n", stats.Errors)
        fmt.Printf("Total Latency: %v\n", time.Duration(stats.TotalLatency))
        if stats.Requests > 0 {
            avgLatency := time.Duration(stats.TotalLatency / stats.Requests)
            fmt.Printf("Average Latency: %v\n", avgLatency)
        }
    }
    ```

### How This Example Demonstrates Concurrency Primitives:

1. **Worker Pool**:
   - Fixed number of workers process requests from a queue
   - Graceful shutdown with quit channels
   - Load balancing across workers

2. **WaitGroup**:
   - Synchronizes the main goroutine with request generators
   - Ensures all requests are submitted before printing stats

3. **Mutexes**:
   - Protects server state (though in this example we use atomics for stats)
   - Could be extended to protect more complex shared state

4. **Atomic Counters**:
   - Thread-safe statistics tracking without mutex overhead
   - Complex atomic operations for custom statistics types
   - Efficient concurrent updates to shared metrics

5. **Combined Usage**:
   - All primitives work together to create a robust concurrent system
   - Each primitive is used for its specific strength
   - Proper error handling and resource management throughout

## Quick Reference

!!! success "Key Takeaways"
    - **Worker Pools**: Use for managing concurrent task processing with controlled resource usage
    - **WaitGroups**: Essential for synchronizing multiple goroutines and waiting for completion
    - **Mutexes**: Protect shared data from race conditions; use RWMutex for read-heavy workloads
    - **Atomic Counters**: Provide lock-free operations for simple numeric types; more efficient than mutexes
    - **Trade-offs**: Choose the right primitive based on operation complexity and performance needs
    - **Performance**: Atomics are fastest for simple operations, mutexes for complex data, worker pools for task distribution
    - **Best Practices**: Always handle errors, avoid deadlocks, keep critical sections short, use defer for cleanup
    - **Combined Usage**: These primitives often work together in real applications for comprehensive concurrency control
    - **Resource Management**: Proper initialization and cleanup are crucial to prevent leaks and ensure correctness

!!! quote "Remember"
    "Go's concurrency primitives provide a powerful toolkit for building efficient and correct concurrent applications. Worker pools help manage resource usage, WaitGroups coordinate goroutine execution, Mutexes protect shared data, and Atomic Counters offer lock-free performance. Understanding when and how to use each primitive is key to mastering Go's concurrency model. The best Go applications combine these primitives thoughtfully, leveraging their strengths while avoiding their pitfalls."